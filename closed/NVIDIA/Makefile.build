# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#	 http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This Makefile contains all the variable and targets related to building the binaries for NVIDIA's MLPerf Inference
# submission. This includes TensorRT plugins, C++ harness binaries, and Triton Inference Server.


include $(CURDIR)/Makefile.const
include $(CURDIR)/Makefile.power

# Specify debug options for build (default to Release build)
ifeq ($(DEBUG),1)
    BUILD_TYPE := Debug
else
    BUILD_TYPE := Release
endif

# Set the include directory for Loadgen header files
INFERENCE_DIR = $(BUILD_DIR)/inference
INFERENCE_URL = https://github.com/mlcommons/inference.git
LOADGEN_INCLUDE_DIR ?= $(INFERENCE_DIR)/loadgen
LOADGEN_LIB_DIR ?= $(LOADGEN_INCLUDE_DIR)/build
# 7/26/2024, last update for checker
INFERENCE_HASH = 379ee5345a25ee123fd5faec33285b6e8be0dadf

# Set Environment variables to extracted contents
export LD_LIBRARY_PATH := $(LD_LIBRARY_PATH):/usr/local/cuda/lib64:/usr/lib/$(ARCH)-linux-gnu:$(LOADGEN_LIB_DIR)
export HARNESS_LD_LIBRARY_PATH := $(LD_LIBRARY_PATH)  # LD_LIBRARY_PATH used during run_harness
export LIBRARY_PATH := /usr/local/cuda/lib64:/usr/lib/$(ARCH)-linux-gnu:$(LOADGEN_LIB_DIR):$(LIBRARY_PATH)
export PATH := /usr/local/cuda/bin:$(PATH)
export CPATH := /usr/local/cuda/include:/usr/include/$(ARCH)-linux-gnu:/usr/include/$(ARCH)-linux-gnu/cub:$(CPATH)
export CUDA_PATH := /usr/local/cuda
export CCACHE_DISABLE=1
export NUMBA_CACHE_DIR=$(BUILD_DIR)/cache

# Set CUDA_DEVICE_MAX_CONNECTIONS to increase multi-stream performance.
export CUDA_DEVICE_MAX_CONNECTIONS := 32

# Set the Triton directory
TRITON_DIR = $(BUILD_DIR)/triton-inference-server
TRITON_OUT_DIR = $(TRITON_DIR)/out
TRITON_PREBUILT_LIBS_DIR = prebuilt_triton_libs
TRITON_URL = https://github.com/triton-inference-server/server
TRITON_TEKIT_URL = https://gitlab-master.nvidia.com/ftp/tekit_backend.git
TRITON_TRTLLM_URL = https://github.com/triton-inference-server/tensorrtllm_backend.git
TRITON_SERVER_COMMIT = mlperf-inf-v4.1
TRITON_HASH = r24.06
TRITON_SERVER_GRPC_FIX = ea3f365c9784660dbe8f533671e977ded94a419c
TRITON_TEKIT_BRANCH = release/0.10
TRITON_TRTLLM_BRANCH = main
# 7/9/2024 Github main
TRITON_TRTLLM_HASH = 97feb8fbca593c6b89eb9d4ed7f016bf37ceb848
TRITON_COMMON_HASH = $(TRITON_HASH)
TRITON_CORE_HASH = $(TRITON_HASH)
TRITON_BACKEND_HASH = $(TRITON_HASH)
TRITON_THIRDPARTY_HASH = $(TRITON_HASH)
TRITON_TENSORRT_HASH = $(TRITON_HASH)
TRITON_PYTHON_BE_HASH = $(TRITON_HASH)

TRT_INCLUDE_DIR = /usr/include/$(ARCH)-linux-gnu/

# Set this to 0 once repo is frozen
CHECK_TRITON_VERSION=0
BYPASS_TRITON_WARNING=0

# FasterTransformer
FT_URL = https://github.com/NVIDIA/FasterTransformer.git
FT_HASH = 35989aa49e47f9ce820032ff08807b8a228f0b68
FT_DIR = $(BUILD_DIR)/FasterTransformer
FT_BUILD_DIR ?= $(FT_DIR)/build
FT_LIB_DIR ?= $(FT_DIR)/build/lib

ifneq (,$(filter $(SM),90 100))
    BUILD_TRTLLM ?= 1
else
	ifneq (, $(filter $(SOC_SM),87))
        BUILD_TRTLLM ?= 1
	else
        BUILD_TRTLLM ?= 0
	endif
endif

# Whether to use public TRTLLM on github
USE_RELEASE_TRTLLM ?= 1
SKIP_TRTLLM_BUILD ?= 0
TRTLLM_DIR ?= $(BUILD_DIR)/TRTLLM
TRTLLM_BUILD_DIR ?= $(BUILD_DIR)/TRTLLM/build
TRTLLM_URL ?= https://gitlab-master.nvidia.com/ftp/tekit.git
TRTLLM_PUBLIC_URL = https://github.com/NVIDIA/TensorRT-LLM.git
# 5/3/2024 ToT (TRT10)
TRTLLM_HASH ?= daec483b860141ea1a0f7f54e4a12c149d1d3bc1
# 7/16/2024 Github mainï¼ˆfinal drop)
TRTLLM_PUBLIC_HASH = 2d234357c6e69fa514f6e9b4d4a5ad3bc431c4a6

ifeq ($(BUILD_CONTEXT), aarch64-Orin)
    TRTLLM_PUBLIC_HASH = dev-sm87-trt101
endif

BUILD_TRITON ?= 0

# Build flags
HARNESS_BUILD_FLAGS := -DCMAKE_BUILD_TYPE=$(BUILD_TYPE) -DLOADGEN_INCLUDE_DIR=$(LOADGEN_INCLUDE_DIR) \
	-DLOADGEN_LIB_DIR=$(LOADGEN_LIB_DIR) -DBUILD_TRTLLM=$(BUILD_TRTLLM) -DTRTLLM_DIR=$(TRTLLM_DIR) \
	-DUSE_RELEASE_TRTLLM=$(USE_RELEASE_TRTLLM) -DTRT_MAJOR_VER=$(TRT_MAJOR_VER)
TRITON_BUILD_FLAGS := --cmake-dir=$(TRITON_DIR) --build-dir=$(TRITON_OUT_DIR) --repo-tag=common:$(TRITON_COMMON_HASH) \
    --repo-tag=core:$(TRITON_CORE_HASH) --repo-tag=backend:$(TRITON_BACKEND_HASH) --repo-tag=thirdparty:$(TRITON_THIRDPARTY_HASH) \
    --backend=tensorrtllm:$(TRITON_TRTLLM_BRANCH) --enable-logging --no-container-build --enable-gpu --endpoint=http --endpoint=grpc --no-force-clone \
    --enable-metrics --enable-stats
ifeq ($(DEBUG),1)
    TRITON_BUILD_FLAGS += --backend=ensemble --enable-tracing --enable-nvtx
endif

ifeq ($(IS_SOC)$(ARCH), 0aarch64)
    BUILD_TRITON := 0
else
    # Jetson
    ifeq ($(IS_SOC), 1)
        BUILD_TRITON := 0
    endif
    HARNESS_BUILD_FLAGS += -DBUILD_TRITON=$(BUILD_TRITON) -DIS_SOC=$(IS_SOC) -DSOC_SM=$(SOC_SM)
    TRITON_BUILD_FLAGS += --build-type=$(BUILD_TYPE)
endif

PLUGINS := NMSOptPlugin pixelShuffle3DPlugin conv3D1X1X1K4Plugin conv3D3X3X3C1K32Plugin retinanetConcatPlugin
ifneq ($(IS_SOC), 1)
    PLUGINS += DLRMv2EmbeddingLookupPlugin
else
    PLUGINS += NMSPVAPlugin
endif
PLUGIN_TARGETS := $(addprefix plugin., $(PLUGINS))


# Add symbolic links to scratch path if it exists.
.PHONY: link_dirs
link_dirs:
	@mkdir -p build
	@mkdir -p $(DATA_DIR)
	@mkdir -p $(PREPROCESSED_DATA_DIR)
	@mkdir -p $(MODEL_DIR)
	@ln -sfn $(DATA_DIR) $(DATA_DIR_LINK)
	@ln -sfn $(PREPROCESSED_DATA_DIR) $(PREPROCESSED_DATA_DIR_LINK)
	@ln -sfn $(MODEL_DIR) $(MODEL_DIR_LINK)


# Clone Triton.
.PHONY: clone_triton
clone_triton:
	 @if [ ! -d $(TRITON_DIR) ]; then \
         echo "Cloning Triton Inference Server" && \
         git clone $(TRITON_URL) $(TRITON_DIR); \
    fi
	@if [ -z "$(TRITON_SERVER_COMMIT)" ]; then \
        TRITON_SERVER_COMMIT=$(TRITON_HASH); \
    fi; \
	cd $(TRITON_DIR) && git fetch && git checkout $(TRITON_SERVER_COMMIT)
	@mkdir -p $(TRITON_OUT_DIR) && \
         git clone -b $(TRITON_TRTLLM_BRANCH) $(TRITON_TRTLLM_URL) $(TRITON_OUT_DIR)/tensorrtllm && \
        cd $(TRITON_OUT_DIR)/tensorrtllm && git checkout $(TRITON_TRTLLM_HASH) && \
        rm -rf $(TRITON_OUT_DIR)/tensorrtllm/tensorrt_llm && \
        ln -s /work/build/TRTLLM/ $(TRITON_OUT_DIR)/tensorrtllm/tensorrt_llm && \
        mkdir -p $(TRITON_OUT_DIR)/trt_link && ln -s $(TRT_INCLUDE_DIR) $(TRITON_OUT_DIR)/trt_link/include
	@if [ -n "$(TRITON_TRTLLM_COMMIT)" ]; then \
        echo "Updating triton TRTLLM backend to $(TRITON_TRTLLM_COMMIT)" && \
        cd $(TRITON_OUT_DIR)/tensorrtllm && git checkout $(TRITON_TRTLLM_COMMIT); \
    fi
	@cd $(TRITON_DIR) && git apply /work/scripts/mlperf_triton_installation.patch
	@cd $(TRITON_OUT_DIR)/tensorrtllm/inflight_batcher_llm && git apply /work/scripts/mlperf_triton_trtllm_backend_installation.patch


# Build all source codes.
.PHONY: build
build: clone_loadgen clone_power_dev link_dirs
ifeq ($(BUILD_TRITON), 1)
	@$(MAKE) -f Makefile.build clone_triton
	@$(MAKE) -f Makefile.build build_triton
endif
ifeq ($(BUILD_TRTLLM), 1)
	@$(MAKE) -f Makefile.build clone_trt_llm
	@$(MAKE) -f Makefile.build build_trt_llm
endif
	@$(MAKE) -f Makefile.build build_plugins
	@$(MAKE) -f Makefile.build build_loadgen
	@$(MAKE) -f Makefile.build build_harness
ifneq ($(IS_SOC), 1)
	@$(MAKE) -f Makefile.build build_faster_transformer
endif


# Clone LoadGen repo.
.PHONY: clone_loadgen
clone_loadgen:
	@if [ ! -d $(LOADGEN_INCLUDE_DIR) ]; then \
		echo "Cloning Official MLPerf Inference (For Loadgen Files)" \
			&& git clone $(INFERENCE_URL) $(INFERENCE_DIR); \
	fi
	@echo "Updating Loadgen" \
		&& cd $(INFERENCE_DIR) \
		&& git fetch \
		&& git checkout $(INFERENCE_HASH) \
		&& git submodule update --init language/bert/DeepLearningExamples


# TODO: Need temporaray fix to support multi-gpu
# NOTE: AARCH64 needs patch for successful build
.PHONY: clone_faster_transformer
clone_faster_transformer:
ifeq ($(filter 89 90,$(SM)),)
	@echo "Skip cloning FasterTransformer for SM $(SM)."
else
	@if [ ! -d $(FT_DIR) ]; then \
		echo "Cloning FasterTransformer" \
			&& git clone $(FT_URL) $(FT_DIR); \
	fi
	@echo "Updating FasterTransformer" \
		&& cd $(FT_DIR) \
		&& git fetch \
		&& git reset --hard HEAD \
		&& git checkout $(FT_HASH) \
		&& echo "Patching FasterTransformer"
ifeq ($(IS_HOPPER),1)
	@if [ $(ARCH) == "aarch64" ]; then \
		echo "For AARCH64" \
			&& cd $(FT_DIR) \
			&& patch -p1 < ../../scripts/ft-build.aarch64.patch; \
	else \
		echo "For x86_64" \
			&& cd $(FT_DIR) \
			&& patch -p1 < ../../scripts/ft-multigpu.patch; \
	fi
endif
endif


.PHONY: clone_trt_llm
clone_trt_llm:
ifeq ($(USE_RELEASE_TRTLLM), 0)
	@if [ ! -d $(TRTLLM_DIR) ]; then \
		echo "Cloning TRT-LLM at ${TRTLLM_URL}" \
			&& git lfs install \
			&& git clone $(TRTLLM_URL) $(TRTLLM_DIR); \
	fi
	@echo "Updating TRT-LLM to $(TRTLLM_HASH)" \
		&& cd $(TRTLLM_DIR) \
		&& git fetch \
		&& git reset --hard HEAD \
		&& git checkout $(TRTLLM_HASH) \
		&& git submodule update --init --recursive
else
	@echo "Cloning public TRT LLM in $(TRTLLM_DIR) from $(TRTLLM_PUBLIC_URL)"
	@if [ ! -d $(TRTLLM_DIR) ]; then \
		echo "Cloning TRT-LLM" \
			&& git clone $(TRTLLM_PUBLIC_URL) $(TRTLLM_DIR); \
	fi
	@echo "Updating TRT-LLM to $(TRTLLM_PUBLIC_HASH)" \
		&& cd $(TRTLLM_DIR) \
		&& git fetch \
		&& git reset --hard HEAD \
		&& git checkout $(TRTLLM_PUBLIC_HASH) \
		&& git submodule update --init --recursive
endif


# Build Triton.
.PHONY: build_triton
build_triton:
	@echo "Building TensorRT Inference Server..."
	@if [ ! -d $(TRITON_DIR) ]; then \
		echo "triton-inference-server does not exist! Please run make clone_triton." \
			&& exit 1; \
	fi
	# Required till triton build.py properly supports incremental builds
	@echo "Build command: ./build.py $(TRITON_BUILD_FLAGS)"
	@rm -rf $(TRITON_OUT_DIR)/tensorrtllm/tensorrt_llm \
		&& ln -s /work/build/TRTLLM/ $(TRITON_OUT_DIR)/tensorrtllm/tensorrt_llm \
		&& cd $(TRITON_DIR) \
		&& ./build.py $(TRITON_BUILD_FLAGS)


# Build FasterTransformer.
.PHONY: build_faster_transformer
build_faster_transformer: clone_faster_transformer
ifeq ($(filter 89 90,$(SM)),)
	@echo "Skip building FasterTransformer for SM $(SM)."
else
	@echo "Building FasterTransformer..."
	@if [ ! -e $(FT_BUILD_DIR) ]; then \
		mkdir $(FT_BUILD_DIR); \
	fi
	sudo update-alternatives --set gcc /usr/bin/gcc-9
	sudo update-alternatives --set g++ /usr/bin/g++-9
ifeq ($(SM), 89)
	@cd $(FT_BUILD_DIR) \
		&& cmake -DSM=89 -DCMAKE_BUILD_TYPE=$(BUILD_TYPE) -DBUILD_TRT=ON -DENABLE_FP8=1 -DUSE_NVTX=$(FT_USE_NVTX) .. \
		&& cmake --build . --target bert_fp8_plugin -j
else ifeq ($(SM), 90)
	@cd $(FT_BUILD_DIR) \
		&& cmake -DSM=90 -DCMAKE_BUILD_TYPE=$(BUILD_TYPE) -DBUILD_TRT=ON -DENABLE_FP8=1 -DUSE_NVTX=$(FT_USE_NVTX) .. \
		&& cmake --build . --target bert_fp8_plugin -j
else
	@echo "Skipping FasterTransformer build for SM $(SM)"
endif
	sudo update-alternatives --set gcc /usr/bin/gcc-11
	sudo update-alternatives --set g++ /usr/bin/g++-11
endif

# Build TRT-LLM wheels and librarys.
.PHONY: build_trt_llm
build_trt_llm:
	@if [ "$(SKIP_TRTLLM_BUILD)" = "1" ]; then \
		echo "Skipping the TRT-LLM build..."; \
		cd $(TRTLLM_DIR) && sed -i '/^#\? *tensorrt/s/^#\?/#/' requirements.txt && pip install -r requirements.txt; \
	else \
		if [ "$(BUILD_CONTEXT)" = "aarch64-Orin" ]; then \
			echo "Building TRTLLM for Orin..."; \
			echo "$(TRTLLM_DIR) Orin..."; \
			cd $(TRTLLM_DIR) && export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1; \
			echo "Building TRT-LLM..."; \
			cd $(TRTLLM_DIR) && python3 scripts/build_wheel.py --cuda_architectures "87-real"; \
			echo "Patching TRTLLM for WAR harness llm build issue..."; \
			git apply /work/scripts/trtllm_orin_installation.patch; \
		else \
			echo "Patching TRTLLM for WAR installation issue..."; \
			cd $(TRTLLM_DIR) && sed -i '/^#\? *tensorrt/s/^#\?/#/' requirements.txt && \
			echo "Building TRT-LLM..."; \
			cd $(TRTLLM_DIR) && python3 scripts/build_wheel.py --clean -a="90-real"; \
		fi; \
	fi
	@echo "Installing TRT-LLM wheel..."
	@pip install $(TRTLLM_BUILD_DIR)/tensorrt*.whl

# Build TensorRT plugins.
.PHONY: build_plugins
build_plugins: $(PLUGIN_TARGETS)


.PHONY: $(PLUGIN_TARGETS)
$(PLUGIN_TARGETS): plugin.%: code/plugin/%
	mkdir -p build/plugins/$(<F)
	cd build/plugins/$(<F)\
		&& cmake -DCMAKE_BUILD_TYPE=$(BUILD_TYPE) $(PROJECT_ROOT)/code/plugin/$(<F) \
		&& make -j


# Build LoadGen.
.PHONY: build_loadgen
build_loadgen:
	@echo "Building loadgen..."
	@if [ ! -e $(LOADGEN_LIB_DIR) ]; then \
		mkdir $(LOADGEN_LIB_DIR); \
	fi
	@cd $(LOADGEN_LIB_DIR) \
		&& cmake -DCMAKE_BUILD_TYPE=$(BUILD_TYPE) .. \
		&& make -j
	@cd $(LOADGEN_INCLUDE_DIR) \
		&& CFLAGS="-std=c++14 -O3" pip wheel . \
		&& pip install --user --force-reinstall *_loadgen-$(subst v,,$(VERSION))-*.whl


# Build harness source codes.
.PHONY: build_harness
build_harness:
	@echo "Building harness..."
	@mkdir -p build/harness \
		&& cd build/harness \
		&& cmake $(HARNESS_BUILD_FLAGS) $(PROJECT_ROOT)/code/harness \
		&& make -j
	@echo "Finished building harness."


# Set proper OC threshold for Jetson submissions
.PHONY: apply_jetson_oc_limit
apply_jetson_oc_limit:
ifeq ($(IS_SOC), 1)
	@sudo cp ./scripts/nvpower_r353_maxp.sh /etc/systemd/nvpower.sh
	@echo "Apply Jetson OC Limit Success."
endif
