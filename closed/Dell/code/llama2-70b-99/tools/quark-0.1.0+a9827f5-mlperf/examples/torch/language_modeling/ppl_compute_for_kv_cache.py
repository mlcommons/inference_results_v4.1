#
# Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
# SPDX-License-Identifier: MIT
#

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM
from typing import Dict, Any
import datetime
import math


def update_model_kwargs_for_generation(
    outputs,
    model_kwargs: Dict[str, Any],
    is_encoder_decoder: bool = False,
) -> Dict[str, Any]:
    """
    Update model arguments for next generation.
    """
    # update past_key_values
    for key in ["past_key_values", "mems", "past_buckets_states"]:
        if key in outputs:
            model_kwargs["past_key_values"] = outputs[key]

    if "state" in outputs:
        model_kwargs["state"] = outputs.state

    if not is_encoder_decoder:
        # update attention mask
        if "attention_mask" in model_kwargs:
            attention_mask = model_kwargs["attention_mask"]
            model_kwargs["attention_mask"] = torch.cat(
                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)

    return model_kwargs


@torch.no_grad()
def get_cumulative_logprob(model: AutoModelForCausalLM, input_tokens: torch.Tensor, future_context: list,
                           dev: str) -> float:
    """
    Calculate the cumulative logprob of the given future context.
    Parameters:
        model : AutoModelForCausalLM
        input_tokens : torch.Tensor
        future_context : list
            A list of token IDs, typically adjacent to the input token. Used for calculating cumulative log probabilities.
        dev : str
            The compute device.
    Returns:
        float
            The cumulative log probabilities of input_tokens and future_context.
    """
    # init
    input_tokens = input_tokens.to(dev)
    model_kwargs = {
        'use_cache': True,
        'attention_mask': torch.ones(input_tokens.shape, dtype=int).to(dev),
    }

    cumulative_logpro = 0
    for next_token in future_context:
        model_inputs = model.prepare_inputs_for_generation(input_tokens, **model_kwargs)
        outputs = model(
            **model_inputs,
            return_dict=True,
            output_hidden_states=True,
        )

        # get the output of the last decoder layer
        hidden_states = outputs.hidden_states[-1].clone().detach()

        # get logits
        selected_token_indices = torch.Tensor([(model_inputs['input_ids'].numel() - 1)]).to(int).to(dev)
        logits = model.lm_head(hidden_states[0, :, :].index_select(0, selected_token_indices))
        logprobs = torch.log_softmax(logits, dim=-1, dtype=torch.float)

        # get the specific token's logprob and accumulate
        cumulative_logpro += logprobs[0, next_token]

        # update for next step
        input_tokens = torch.cat((input_tokens, torch.tensor([
            [
                next_token,
            ],
        ]).to(dev)), dim=-1)
        model_kwargs = update_model_kwargs_for_generation(outputs,
                                                          model_kwargs,
                                                          is_encoder_decoder=model.config.is_encoder_decoder)
    return cumulative_logpro.item()


@torch.no_grad()
def ppl_eval_for_kv_cache(model: nn.Module, testenc: torch.Tensor, context_size: int, sample_size: int, patch_size: int,
                          dev: str) -> None:
    """
    A perplexity-computing test for the KV cache system.
    Parameters:
        model : nn.Module
        testenc : torch.Tensor
            The input token IDs.
        context_size : int
            The size of the context used for generation.
        sample_size : int
            The number of the output generated by the given context size. This his variable also \
            defines the size of the individual patch.
        patch_size : int
            The size of patches, if specified, will determine the number of patches. If not, \
            the number of patches will be determined by sample_size and the number of input tokens.
        dev : str
    Returns:
        None
    """
    print(f"Initializing @ {datetime.datetime.now()}")

    # Prepare parameters
    my_enc = testenc.input_ids
    n_samples = sample_size
    n_patches = math.ceil((my_enc.numel() - context_size - 1) / n_samples)
    if patch_size is not None:
        n_patches = patch_size

    ppl = 0.0
    num_tokens_generated = 0
    starting_time = datetime.datetime.now()
    print(f'Starting generation @ {starting_time} '
          f'will try to process {n_patches} patch(es), '
          f'generating {n_samples} tokens in each patch '
          f'from the initial context of {context_size} tokens.')
    for idx in range(n_patches):
        context = my_enc[:, idx * n_samples:idx * n_samples + context_size]
        upper_boundary = min((idx + 1) * n_samples + context_size, my_enc.numel())
        future_context = my_enc[0, idx * n_samples + context_size:upper_boundary].tolist()

        logprobs = get_cumulative_logprob(model=model, input_tokens=context, future_context=future_context, dev=dev)

        ppl -= logprobs
        num_tokens_generated += len(future_context)

        print(f'Iteration {idx+1} of {n_patches} Intermediate '
              'Estimates:\n'
              f'\tCross-entropy_intermediate={ppl/num_tokens_generated}\n'
              f'\tPerplexity_intermediate={math.exp(ppl/num_tokens_generated)}')

    ending_time = datetime.datetime.now()
    print(f'Done @ {ending_time} after processing for '
          f'{ending_time-starting_time} generated {num_tokens_generated} tokens.')
    print(f'Integral Cross-Entropy={ppl} Average Cross-Entropy='
          f'{ppl/num_tokens_generated} PPL={math.exp(ppl/num_tokens_generated)}')
