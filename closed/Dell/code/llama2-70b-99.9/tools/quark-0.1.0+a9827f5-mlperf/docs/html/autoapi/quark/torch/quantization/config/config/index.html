<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>quark.torch.quantization.config.config &mdash; Quark  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/graphviz.css" />

  
  <!--[if lt IE 9]>
    <script src="../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../../../../" id="documentation_options" src="../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../_static/doctools.js"></script>
        <script src="../../../../../../_static/sphinx_highlight.js"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            Quark
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../md_sources/quark_torch_main_gen.html">Quark for Pytorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../md_sources/install.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../md_sources/user_guide_gen.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../api_doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../md_sources/example_gen.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../md_sources/faq.html">FAQ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">Quark</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">quark.torch.quantization.config.config</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/autoapi/quark/torch/quantization/config/config/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-quark.torch.quantization.config.config">
<span id="quark-torch-quantization-config-config"></span><h1>quark.torch.quantization.config.config<a class="headerlink" href="#module-quark.torch.quantization.config.config" title="Permalink to this heading"></a></h1>
<p>Quark Quantization Config API for PyTorch</p>
<section id="classes">
<h2>Classes<a class="headerlink" href="#classes" title="Permalink to this heading"></a></h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#quark.torch.quantization.config.config.Config" title="quark.torch.quantization.config.config.Config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Config</span></code></a></p></td>
<td><p>A class that encapsulates comprehensive quantization configurations for a machine learning model, allowing for detailed and hierarchical control over quantization parameters across different model components.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#quark.torch.quantization.config.config.QuantizationConfig" title="quark.torch.quantization.config.config.QuantizationConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizationConfig</span></code></a></p></td>
<td><p>A data class that specifies quantization configurations for different components of a module, allowing hierarchical control over how each tensor type is quantized.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#quark.torch.quantization.config.config.QuantizationSpec" title="quark.torch.quantization.config.config.QuantizationSpec"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantizationSpec</span></code></a></p></td>
<td><p>A data class that defines the specifications for quantizing tensors within a model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#quark.torch.quantization.config.config.SmoothQuantConfig" title="quark.torch.quantization.config.config.SmoothQuantConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SmoothQuantConfig</span></code></a></p></td>
<td><p>A data class that defines the specifications for Smooth Quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#quark.torch.quantization.config.config.AWQConfig" title="quark.torch.quantization.config.config.AWQConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AWQConfig</span></code></a></p></td>
<td><p>Configuration for Activation-aware Weight Quantization (AWQ).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#quark.torch.quantization.config.config.GPTQConfig" title="quark.torch.quantization.config.config.GPTQConfig"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GPTQConfig</span></code></a></p></td>
<td><p>A data class that defines the specifications for Accurate Post-Training Quantization for Generative Pre-trained Transformers (GPTQ).</p></td>
</tr>
</tbody>
</table>
</section>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="quark.torch.quantization.config.config.Config">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">quark.torch.quantization.config.config.</span></span><span class="sig-name descname"><span class="pre">Config</span></span><a class="headerlink" href="#quark.torch.quantization.config.config.Config" title="Permalink to this definition"></a></dt>
<dd><p>A class that encapsulates comprehensive quantization configurations for a machine learning model, allowing for detailed and hierarchical control over quantization parameters across different model components.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>global_quant_config</strong> (<a class="reference internal" href="#quark.torch.quantization.config.config.QuantizationConfig" title="quark.torch.quantization.config.config.QuantizationConfig"><em>QuantizationConfig</em></a>) – Global quantization configuration applied to the entire model unless overridden at the layer level.</p></li>
<li><p><strong>layer_type_quant_config</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><a class="reference internal" href="#quark.torch.quantization.config.config.QuantizationConfig" title="quark.torch.quantization.config.config.QuantizationConfig"><em>QuantizationConfig</em></a><em>]</em>) – A dictionary mapping from layer types (e.g., ‘Conv2D’, ‘Dense’) to their quantization configurations. Default is an empty dictionary.</p></li>
<li><p><strong>layer_quant_config</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><a class="reference internal" href="#quark.torch.quantization.config.config.QuantizationConfig" title="quark.torch.quantization.config.config.QuantizationConfig"><em>QuantizationConfig</em></a><em>]</em>) – A dictionary mapping from layer names to their quantization configurations, allowing for per-layer customization. Default is an empty dictionary.</p></li>
<li><p><strong>exclude</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – A list of layer names to be excluded from quantization, enabling selective quantization of the model. Default is an empty list.</p></li>
<li><p><strong>algo_config</strong> (<em>Optional</em><em>[</em><em>AlgoConfig</em><em>]</em>) – Optional configuration for the quantization algorithm, such as GPTQ and AWQ. After this process, the datatype/fake_datatype of weights will be changed with quantization scales. Default is None.</p></li>
<li><p><strong>pre_quant_opt_config</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>PreQuantOptConfig</em><em>, </em><em>List</em><em>[</em><em>PreQuantOptConfig</em><em>]</em><em>]</em><em>]</em>) – Optional pre-processing optimization, such as Equalization and SmoothQuant. After this process, the value of weights will be changed, but the dtype/fake_dtype will be the same. Default is None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quark.torch.quantization.config.config.QuantizationConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">quark.torch.quantization.config.config.</span></span><span class="sig-name descname"><span class="pre">QuantizationConfig</span></span><a class="headerlink" href="#quark.torch.quantization.config.config.QuantizationConfig" title="Permalink to this definition"></a></dt>
<dd><p>A data class that specifies quantization configurations for different components of a module, allowing hierarchical control over how each tensor type is quantized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_tensors</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#quark.torch.quantization.config.config.QuantizationSpec" title="quark.torch.quantization.config.config.QuantizationSpec"><em>QuantizationSpec</em></a><em>]</em>) – Input tensors quantization specification. If None, following the hierarchical quantization setup. e.g. If the input_tensors in layer_type_quant_config is None, the configuration from global_quant_config will be used instead. Defaults to None. If None in global_quant_config, input_tensors are not quantized.</p></li>
<li><p><strong>output_tensors</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#quark.torch.quantization.config.config.QuantizationSpec" title="quark.torch.quantization.config.config.QuantizationSpec"><em>QuantizationSpec</em></a><em>]</em>) – Output tensors quantization specification. Defaults to None. If None, the same as above.</p></li>
<li><p><strong>weight</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#quark.torch.quantization.config.config.QuantizationSpec" title="quark.torch.quantization.config.config.QuantizationSpec"><em>QuantizationSpec</em></a><em>]</em>) – The weights tensors quantization specification. Defaults to None. If None, the same as above.</p></li>
<li><p><strong>bias</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#quark.torch.quantization.config.config.QuantizationSpec" title="quark.torch.quantization.config.config.QuantizationSpec"><em>QuantizationSpec</em></a><em>]</em>) – The bias tensors quantization specification. Defaults to None. If None, the same as above.</p></li>
<li><p><strong>target_device</strong> (<em>Optional</em><em>[</em><em>DeviceType</em><em>]</em>) – Configuration specifying the target device (e.g., CPU, GPU, IPU) for the quantized model.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quark.torch.quantization.config.config.QuantizationSpec">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">quark.torch.quantization.config.config.</span></span><span class="sig-name descname"><span class="pre">QuantizationSpec</span></span><a class="headerlink" href="#quark.torch.quantization.config.config.QuantizationSpec" title="Permalink to this definition"></a></dt>
<dd><p>A data class that defines the specifications for quantizing tensors within a model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<em>Dtype</em>) – The data type for quantization (e.g., int8, int4).</p></li>
<li><p><strong>is_dynamic</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Specifies whether dynamic or static quantization should be used. Default is None, which indicates no specification.</p></li>
<li><p><strong>observer_cls</strong> (<em>Optional</em><em>[</em><em>Type</em><em>[</em><em>ObserverBase</em><em>]</em><em>]</em>) – The class of observer to be used for determining quantization parameters like min/max values. Default is None.</p></li>
<li><p><strong>qscheme</strong> (<em>Optional</em><em>[</em><em>QSchemeType</em><em>]</em>) – The quantization scheme to use, such as per_tensor, per_channel or per_group. Default is None.</p></li>
<li><p><strong>ch_axis</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The channel axis for per-channel quantization. Default is None.</p></li>
<li><p><strong>group_size</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The size of the group for per-group quantization. Default is None.</p></li>
<li><p><strong>symmetric</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Indicates if the quantization should be symmetric around zero. If True, quantization is symmetric. If None, it defers to a higher-level or global setting. Default is None.</p></li>
<li><p><strong>round_method</strong> (<em>Optional</em><em>[</em><em>RoundType</em><em>]</em>) – The rounding method during quantization, such as half_even. If None, it defers to a higher-level or default method. Default is None.</p></li>
<li><p><strong>scale_type</strong> (<em>Optional</em><em>[</em><em>ScaleType</em><em>]</em>) – Defines the scale type to be used for quantization, like power of two or float. If None, it defers to a higher-level setting or uses a default method. Default is None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quark.torch.quantization.config.config.SmoothQuantConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">quark.torch.quantization.config.config.</span></span><span class="sig-name descname"><span class="pre">SmoothQuantConfig</span></span><a class="headerlink" href="#quark.torch.quantization.config.config.SmoothQuantConfig" title="Permalink to this definition"></a></dt>
<dd><p>A data class that defines the specifications for Smooth Quantization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – The name of the configuration, typically used to identify different quantization settings. Default is “smoothquant”.</p></li>
<li><p><strong>alpha</strong> (<em>int</em>) – The factor of adjustment in the quantization formula, influencing how aggressively weights are quantized. Default is 1.</p></li>
<li><p><strong>scale_clamp_min</strong> (<em>float</em>) – The minimum scaling factor to be used during quantization, preventing the scale from becoming too small. Default is 1e-3.</p></li>
<li><p><strong>scaling_layers</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>str</em><em>]</em><em>]</em><em>]</em>) – Specific settings for scaling layers, allowing customization of quantization parameters for different layers within the model. Default is None.</p></li>
<li><p><strong>embedding_layers</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – A list of embedding layer names that require special quantization handling to maintain their performance and accuracy. Default is None.</p></li>
<li><p><strong>model_decoder_layers</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Specifies any particular decoder layers in the model that might have unique quantization requirements. Default is None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quark.torch.quantization.config.config.AWQConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">quark.torch.quantization.config.config.</span></span><span class="sig-name descname"><span class="pre">AWQConfig</span></span><a class="headerlink" href="#quark.torch.quantization.config.config.AWQConfig" title="Permalink to this definition"></a></dt>
<dd><p>Configuration for Activation-aware Weight Quantization (AWQ).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – The name of the quantization configuration. Default is “awq”.</p></li>
<li><p><strong>bit</strong> (<em>int</em>) – The bit width for weights, indicating the precision of quantization. Defaults to 4 bits.</p></li>
<li><p><strong>sym</strong> (<em>bool</em>) – Indicates whether symmetric quantization should be used. If True, quantization is symmetric around zero. Default is False.</p></li>
<li><p><strong>group_size</strong> (<em>int</em>) – The size of the group for grouped quantization, specifying how many weights are quantized together using the same scale and zero-point. Default is 128.</p></li>
<li><p><strong>algo_processor</strong> (<em>Type</em><em>[</em><em>AwqProcessor</em><em>]</em>) – The processor type that handles the AWQ algorithm logic.</p></li>
<li><p><strong>scaling_layers</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>str</em><em>]</em><em>]</em><em>]</em>) – Configuration details for scaling layers within the model, specifying custom scaling parameters per layer. Default is None.</p></li>
<li><p><strong>model_decoder_layers</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Specifies the layers involved in model decoding that may require different quantization parameters. Default is None.</p></li>
<li><p><strong>embedding_layers</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – Lists the embedding layers within the model that need to be quantized separately. Default is None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="quark.torch.quantization.config.config.GPTQConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">quark.torch.quantization.config.config.</span></span><span class="sig-name descname"><span class="pre">GPTQConfig</span></span><a class="headerlink" href="#quark.torch.quantization.config.config.GPTQConfig" title="Permalink to this definition"></a></dt>
<dd><p>A data class that defines the specifications for Accurate Post-Training Quantization for Generative Pre-trained Transformers (GPTQ).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – The configuration name. Default is “gptq”.</p></li>
<li><p><strong>bit</strong> (<em>int</em>) – The bit width for quantization, indicating the precision level of the quantized values. Defaults to 4 bits.</p></li>
<li><p><strong>sym</strong> (<em>bool</em>) – Specifies whether symmetric quantization is used. Symmetric quantization centers the quantized values around zero. Default is False.</p></li>
<li><p><strong>group_size</strong> (<em>int</em>) – Specifies the number of weights to be quantized together as a group, using the same scale and zero-point. Default is 128.</p></li>
<li><p><strong>damp_percent</strong> (<em>float</em>) – The percentage used to dampen the quantization effect, aiding in the maintenance of accuracy post-quantization. Default is 0.01.</p></li>
<li><p><strong>desc_act</strong> (<em>bool</em>) – Indicates whether descending activation is used, typically to enhance model performance with quantization. Default is True.</p></li>
<li><p><strong>static_groups</strong> (<em>bool</em>) – Specifies whether the groups for quantization are static (fixed during initialization) or can be dynamically adjusted during training. Default is False.</p></li>
<li><p><strong>true_sequential</strong> (<em>bool</em>) – Indicates whether the quantization should be applied in a truly sequential manner across the layers. Default is True.</p></li>
<li><p><strong>inside_layer_modules</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – Lists the names of internal layer modules within the model that require specific quantization handling. Default is None.</p></li>
<li><p><strong>model_decoder_layers</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Specifies custom settings for quantization on specific decoder layers of the model. Default is None.</p></li>
<li><p><strong>embedding_layers</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – Identifies which embedding layers within the model need to be quantized separately to preserve embedding quality. Default is None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Advanced Micro Devices, Inc. All rights reserved.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>