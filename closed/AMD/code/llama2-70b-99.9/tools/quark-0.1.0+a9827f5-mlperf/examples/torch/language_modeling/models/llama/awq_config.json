{
    "scaling_layers":[
        {
            "prev_op": "input_layernorm",
            "layers": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj"],
            "inp": "self_attn.q_proj",
            "module2inspect": "self_attn",
            "has_kwargs": true,
            "help": "attention input"
        },
        {
            "prev_op": "self_attn.v_proj",
            "layers": ["self_attn.o_proj"],
            "inp": "self_attn.o_proj",
            "module2inspect": null,
            "has_kwargs": false,
            "help": "attention out, Please refer to https://github.com/mit-han-lab/llm-awq/pull/67#issue-1850622696, if module.self_attn.v_proj.weight.shape == module.self_attn.o_proj.weight.shape",
            "condition": "module.self_attn.v_proj.weight.shape == module.self_attn.o_proj.weight.shape"
        },
        {
            "prev_op": "post_attention_layernorm",
            "layers": ["mlp.gate_proj", "mlp.up_proj"],
            "inp": "mlp.gate_proj",
            "module2inspect": "mlp",
            "has_kwargs": false,
            "help": "linear 1"
        },
        {
            "prev_op": "mlp.up_proj",
            "layers": ["mlp.down_proj"],
            "inp": "mlp.down_proj",
            "module2inspect": null,
            "has_kwargs": false,
            "help": "linear 2"
        }
    ],
    "model_decoder_layers": "model.layers",
    "embedding_layers":["model.embed_tokens"]
}
